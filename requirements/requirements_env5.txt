# Core ML Framework - let vLLM control torch version for compatibility
# torch  # commented out - vllm will pull the right version
triton
setuptools  # required by triton

# Transformers and NLP
transformers
tokenizers
safetensors
accelerate
bitsandbytes

# LangChain ecosystem
langchain
langchain-core
langchain-text-splitters
langsmith

# ML/Data Science
numpy
scipy
networkx

# API and Web
fastapi
uvicorn[standard]
uvloop
httpx
httpcore
httptools
aiohttp
starlette
websockets

# Database
sqlalchemy

# Cloud/AI Services
huggingface-hub
hf-xet

# Monitoring and system
gpustat
blessed
nvidia-ml-py
nvidia-ml-py3
psutil

# Utilities
requests
requests-toolbelt
pyyaml
python-dotenv
python-multipart
click
tqdm
regex

# Data handling
pydantic
pydantic_core
packaging
filelock
fsspec

# Math and symbolic
sympy
mpmath

# Testing
pytest
pytest-asyncio
pluggy
iniconfig

# Async and networking
anyio
aiohappyeyeballs
aiosignal
async-timeout
frozenlist
multidict
propcache
yarl

# Performance and serialization
orjson
zstandard
jsonpatch
jsonpointer

# Other utilities
attrs
certifi
charset-normalizer
idna
urllib3
h11
sniffio
exceptiongroup
tenacity
greenlet
jinja2
markupsafe
pygments
watchfiles
typing_extensions
typing-inspection
wcwidth
tomli
backports.asyncio.runner

# vLLM (use pre-built wheels)
# Pin to 0.6.x-0.8.x for torch 2.4/2.5 compatibility with flash-attn kernels
vllm>=0.6.0,<0.9.0